{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Get to the Point \n",
    "\n",
    "This notebook implements model proposed in the paper: [Get to the Point](https://arxiv.org/abs/1704.04368) which is probably one of the most famous paper during that time.  Even today, people is using this pointer-generator architecture as one component in their model.    Btw, if you forget, you may want to read our lecture where we discuss this paper, or you can also encourage to read the paper once before doing this together.\n",
    "\n",
    "Source:\n",
    "- https://github.com/abisee/pointer-generator\n",
    "- https://github.com/atulkum/pointer_summarizer\n",
    "- https://github.com/laihuiyuan/pointer-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 09:51:03.040781: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import csv, queue, random, time, glob, struct, time, os, math\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.core.example import example_pb2\n",
    "from threading import Thread\n",
    "import logging\n",
    "\n",
    "import pyrouge\n",
    "\n",
    "logger = logging.getLogger('chaky_logger')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_STA = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "UNK = 0\n",
    "PAD = 1\n",
    "BOS = 2\n",
    "EOS = 3\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "BOS_TOKEN = '[BOS]'\n",
    "EOS_TOKEN = '[EOS]'\n",
    "\n",
    "beam_size=4\n",
    "emb_dim= 128\n",
    "batch_size= 16\n",
    "hidden_dim= 256\n",
    "max_enc_steps=400\n",
    "max_dec_steps=100\n",
    "max_tes_steps=100\n",
    "min_dec_steps=35\n",
    "vocab_size=50000\n",
    "\n",
    "lr=0.15\n",
    "cov_loss_wt = 1.0\n",
    "pointer_gen = True\n",
    "is_coverage = True\n",
    "\n",
    "max_grad_norm=2.0\n",
    "adagrad_init_acc=0.1\n",
    "rand_unif_init_mag=0.02\n",
    "trunc_norm_init_std=1e-4\n",
    "\n",
    "eps = 1e-12\n",
    "use_gpu=True\n",
    "lr_coverage=0.15\n",
    "max_iterations = 5 #500000\n",
    "\n",
    "log_root = \"data/log/\"\n",
    "\n",
    "train_data_path  = \"data/finished_files/chunked/train_*\"\n",
    "eval_data_path   = \"data/finished_files/val.bin\"\n",
    "decode_data_path = \"data/finished_files/test.bin\"\n",
    "vocab_path       = \"data/finished_files/vocab\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download data\n",
    "\n",
    "Go to https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail and download `FINISHED_FILES`.  This is basically tokenized form of cnn news summaries and dailymail summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Since things are tokenized, we can skip!  Yay!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numericalize\n",
    "\n",
    "We gonna load our data, check for unique vocabs, and numericalize them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help our numericalization, we gonna made the `Vocab` object that gonna help us handle things.  Most of the code is quite self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, file, max_size):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.count = 0     # keeps track of total number of words in the Vocab\n",
    "\n",
    "        # [UNK], [PAD], [BOS] and [EOS] get the ids 0,1,2,3.\n",
    "        for w in [UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]:\n",
    "            self.word2idx[w] = self.count\n",
    "            self.idx2word[self.count] = w\n",
    "            self.count += 1\n",
    "\n",
    "        # Read the vocab file and add words up to max_size\n",
    "        with open(file, 'r') as fin:\n",
    "            for line in fin:\n",
    "                items = line.split()\n",
    "                if len(items) != 2:\n",
    "                    print('Warning: incorrectly formatted line in vocabulary file: %s' % line.strip())\n",
    "                    continue\n",
    "                w = items[0]\n",
    "                if w in [SENTENCE_STA, SENTENCE_END, UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]:\n",
    "                    raise Exception(\n",
    "                        '<s>, </s>, [UNK], [PAD], [BOS] and [EOS] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "                if w in self.word2idx:\n",
    "                    raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "                self.word2idx[w] = self.count\n",
    "                self.idx2word[self.count] = w\n",
    "                self.count += 1\n",
    "                if max_size != 0 and self.count >= max_size:\n",
    "                    break\n",
    "        print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (\n",
    "          self.count, self.idx2word[self.count - 1]))\n",
    "\n",
    "    def word2id(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx[UNK_TOKEN]\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def id2word(self, word_id):\n",
    "        if word_id not in self.idx2word:\n",
    "            raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "        return self.idx2word[word_id]\n",
    "\n",
    "    def size(self):\n",
    "        return self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Vocab class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
      "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
      "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
      "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
      "Finished constructing vocabulary of 50000 total words. Last word added: perisic\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(vocab_path, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word2id(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.id2word(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare batch loader\n",
    "\n",
    "### 4.1 Example\n",
    "\n",
    "First we will write the `Example` class.  A very simple class that will help hold a pair of article and the label abstract.  Just one pair (thus we called `Example`).\n",
    "\n",
    "Here, \n",
    "1. We first decode both the article and abstract (from bytes to strings) (bytes are store in .bin which is a more efficient way)\n",
    "2. We create the `enc_inp` to store the encoder input from the article, which is simply ids which we convert by using the `vocab.word2id`\n",
    "3. We create the `dec_inp` and `tgt` from the abstract which is also ids which we convert through `vocab.word2id`.  The difference of `dec_inp` and `tgt` is that they are one words away, i.e., `tgt` will have `SOS token` added in front, for decoding purpose\n",
    "4. If we plan to use the pointer and generator mode, we need to store a version of the `enc_input` and `tgt` which knows the oov and assign unique id for each oov.  Later it will make sense.\n",
    "\n",
    "Note that both `enc_inp`, `dec_inp` and `tgt` are truncated based on `max_en_steps` and `max_dec_steps` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Example(object):\n",
    "\n",
    "    def __init__(self, article, abstract_sentences, vocab):\n",
    "        # Get ids of special tokens\n",
    "        bos_decoding = vocab.word2id(BOS_TOKEN)\n",
    "        eos_decoding = vocab.word2id(EOS_TOKEN)\n",
    "\n",
    "        # Process the article\n",
    "        article_words = article.decode().split()\n",
    "        if len(article_words) > max_enc_steps:\n",
    "            article_words = article_words[:max_enc_steps]\n",
    "        self.enc_len = len(article_words)  # store the length after truncation but before padding\n",
    "        self.enc_inp = [vocab.word2id(w) for w in\n",
    "                          article_words]   # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "        # Process the abstract, converting bytes to unicode\n",
    "        abstract = ' '.encode().join(abstract_sentences).decode()        \n",
    "        abstract_words = abstract.split()  # list of strings\n",
    "        abs_ids = [vocab.word2id(w) for w in abstract_words]  # list of word ids; OOVs are represented by the id for UNK token\n",
    "\n",
    "        # Get the decoder input sequence and target sequence\n",
    "        self.dec_inp, self.tgt = self.get_dec_seq(abs_ids, max_dec_steps, bos_decoding, eos_decoding)        \n",
    "        self.dec_len = len(self.dec_inp)\n",
    "\n",
    "        # If using pointer-generator mode, we need to store some extra info\n",
    "        if pointer_gen:\n",
    "            # Store a version of the enc_input where in-article OOVs are represented by their temporary OOV id;\n",
    "            # also store the in-article OOVs words themselves\n",
    "            self.enc_inp_extend_vocab, self.article_oovs = article2ids(article_words, vocab)\n",
    "\n",
    "            # Get a verison of the reference summary where in-article OOVs are represented by their temporary article OOV id\n",
    "            abs_ids_extend_vocab = abstract2ids(abstract_words, vocab, self.article_oovs)\n",
    "\n",
    "            # Overwrite decoder target sequence so it uses the temp article OOV ids\n",
    "            _, self.tgt = self.get_dec_seq(abs_ids_extend_vocab, max_dec_steps, bos_decoding, eos_decoding)\n",
    "\n",
    "        # Store the original strings\n",
    "        self.original_article  = article\n",
    "        self.original_abstract = abstract\n",
    "        self.original_abstract_sents = abstract_sentences\n",
    "\n",
    "    def get_dec_seq(self, sequence, max_len, start_id, stop_id):\n",
    "        src = [start_id] + sequence[:]\n",
    "        tgt = sequence[:]\n",
    "        if len(src) > max_len:   # truncate\n",
    "            src = src[:max_len]\n",
    "            tgt = tgt[:max_len]  # no end_token\n",
    "        else:  # no truncation\n",
    "            tgt.append(stop_id)  # end token\n",
    "        assert len(src) == len(tgt)\n",
    "        return src, tgt\n",
    "\n",
    "    #this is for padding; used for Batch class....\n",
    "    #basically take the max_len of the batch, and pad all sentences to that length\n",
    "    def pad_enc_seq(self, max_len, pad_id):\n",
    "        while len(self.enc_inp) < max_len:\n",
    "            self.enc_inp.append(pad_id)\n",
    "        if pointer_gen:\n",
    "            while len(self.enc_inp_extend_vocab) < max_len:\n",
    "                self.enc_inp_extend_vocab.append(pad_id)\n",
    "\n",
    "    #same\n",
    "    def pad_dec_seq(self, max_len, pad_id):\n",
    "        while len(self.dec_inp) < max_len:\n",
    "            self.dec_inp.append(pad_id)\n",
    "        while len(self.tgt) < max_len:\n",
    "            self.tgt.append(pad_id)\n",
    "            \n",
    "### utility functions\n",
    "\n",
    "#converting abstract to ids\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNK_TOKEN)\n",
    "    for w in abstract_words:\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id:  # If w is an OOV word\n",
    "            if w in article_oovs:  # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w)  # Map to its temporary article OOV number\n",
    "                ids.append(vocab_idx)\n",
    "            else:  # If w is an out-of-article OOV\n",
    "                ids.append(unk_id)  # Map to the UNK token id\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "\n",
    "#converting article to ids\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oov = []\n",
    "    unk_id = vocab.word2id(UNK_TOKEN)\n",
    "    for w in article_words:\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id:  # If w is OOV\n",
    "            if w not in oov:  # Add to list of OOVs\n",
    "                oov.append(w)\n",
    "            oov_num = oov.index(w)  # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            ids.append(vocab.size() + oov_num)  # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oov\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Example\n",
    "\n",
    "Our code used bytes for efficiency.  When we want to convert it back to unicode, we use `decode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = b\"the real wolf Chaky of wall has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . jordan belfort , who is played by leonardo dicaprio in the oscar-nominated film , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars of savings . ` it was about a month in when i realized . that was the first moment where i allowed greed to get the better of me , ' he told piers morgan live . scroll down for video . soulless : the real wolf of wall street , jordan belfort , pictured monday night , has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . belfort , 51 , who is now a motivational speaker , applauded the movie , and said he spent more than 100 hours with dicaprio , helping him prepare for the role of his life . he praised the actor for making the film what he calls a cautionary tale rather than glorifying the excessive behavior , as many critics have claimed the film does .\"\n",
    "type(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_sentences = [b'jordan belfort , Chaky who is played by leonardo dicaprio , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars .', b'belfort , 51 , who is now a motivational speaker , applauded the wolf of wall street , but said it was difficult to watch in parts .', b'he said he spent more than 100 hours helping dicaprio prepare for the role of his life .', b'the man who spent 22 months in prison for securities fraud and money laundering insisted to piers morgan that 95 per cent of what he did was legal .', b\"but he added ` the 5 per cent was incredibly destructive and it poisoned everything '\"]\n",
    "len(abstract_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = Example(article, abstract_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the real wolf [UNK] of wall has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . jordan belfort , who is played by leonardo dicaprio in the oscar-nominated film , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars of savings . ` it was about a month in when i realized . that was the first moment where i allowed greed to get the better of me , ' he told piers morgan live . scroll down for video . soulless : the real wolf of wall street , jordan belfort , pictured monday night , has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . belfort , 51 , who is now a motivational speaker , applauded the movie , and said he spent more than 100 hours with dicaprio , helping him prepare for the role of his life . he praised the actor for making the film what he calls a cautionary tale rather than glorifying the excessive behavior , as many critics have claimed the film does . "
     ]
    }
   ],
   "source": [
    "#looking at encoder input\n",
    "for id in example.enc_inp:\n",
    "    print(vocab.id2word(id), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOS] jordan belfort , [UNK] who is played by leonardo dicaprio , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars . belfort , 51 , who is now a motivational speaker , applauded the wolf of wall street , but said it was difficult to watch in parts . he said he spent more than 100 hours helping dicaprio prepare for the role of his life . the man who spent 22 months in prison for securities fraud and money laundering insisted to piers "
     ]
    }
   ],
   "source": [
    "#looking at decoder input (it's truncated based on max_dec_steps)\n",
    "for id in example.dec_inp:\n",
    "    print(vocab.id2word(id), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jordan belfort , Unknown id 50000\n",
      "who is played by leonardo dicaprio , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars . belfort , 51 , who is now a motivational speaker , applauded the wolf of wall street , but said it was difficult to watch in parts . he said he spent more than 100 hours helping dicaprio prepare for the role of his life . the man who spent 22 months in prison for securities fraud and money laundering insisted to piers morgan "
     ]
    }
   ],
   "source": [
    "#looking at tgt (it's basically one more ahead of input, since we are using teaching forcing)\n",
    "for id in example.tgt:\n",
    "    try:\n",
    "        print(vocab.id2word(id), end=\" \")\n",
    "    except:\n",
    "        print(\"Unknown id\", id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chaky']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at our the example know which is the oov in article\n",
    "example.article_oovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#any oov will get id 50000, 500001, etc.  Since I have only Chaky, it is 50000\n",
    "example.enc_inp_extend_vocab[3] #belongs to \"Chaky\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if the tgt also has Chaky, it will also assign the same id 50000\n",
    "example.tgt[3] #belongs to Chaky"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Batch\n",
    "\n",
    "Let's create a class `Batch` for holding a list of `Example`.\n",
    "\n",
    "In this class, what it will do is:\n",
    "\n",
    "1.  Determine the max length of sentence in the batch\n",
    "2.  Then pad according to this max length\n",
    "3.  Then create empty numpy arrays of shape `enc_batch`:`(batch_size, max_len)`, `enc_lens`: `(batch_size, )`, and `enc_padding_mask`:`(batch_size, max_len)` and populate accordingly\n",
    "4.  This is done in the same manner for `dec_batch`, `tgt_batch`, `dec_padding_mask`, and `dec_lens`\n",
    "\n",
    "Note that `lens` are needed when we do padding, so that the model knows the original length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch(object):\n",
    "    def __init__(self, example_list, vocab, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_id = vocab.word2id(PAD_TOKEN)  # id of the PAD token used to pad sequences\n",
    "        self.init_encoder_seq(example_list)  # initialize the input to the encoder\n",
    "        self.init_decoder_seq(example_list)  # initialize the input and targets for the decoder\n",
    "        self.store_orig_strings(example_list)  # store the original strings\n",
    "\n",
    "    def init_encoder_seq(self, example_list):\n",
    "        # Determine the maximum length of the encoder input sequence in this batch\n",
    "        max_enc_seq_len = max([ex.enc_len for ex in example_list])\n",
    "\n",
    "        # Pad the encoder input sequences up to the length of the longest sequence\n",
    "        for ex in example_list:\n",
    "            ex.pad_enc_seq(max_enc_seq_len, self.pad_id)\n",
    "\n",
    "        # Initialize the numpy arrays\n",
    "        # Note: our enc_batch can have different length (second dimension) for each batch because we use dynamic_rnn for the encoder.\n",
    "        self.enc_batch = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "        self.enc_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "        self.enc_padding_mask = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.float32)  #basically [1, 1, 1, 1, 1, 0, 0, ..] where 0 indicates that that pos is a padding\n",
    "\n",
    "        # Fill in the numpy arrays\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.enc_batch[i, :] = ex.enc_inp[:]\n",
    "            self.enc_lens[i] = ex.enc_len\n",
    "            for j in range(ex.enc_len):\n",
    "                self.enc_padding_mask[i][j] = 1\n",
    "\n",
    "        # For pointer-generator mode, need to store some extra info\n",
    "        if pointer_gen:\n",
    "            # Determine the max number of in-article OOVs in this batch\n",
    "            self.max_art_oovs = max([len(ex.article_oovs) for ex in example_list])\n",
    "            # Store the in-article OOVs themselves\n",
    "            self.art_oovs = [ex.article_oovs for ex in example_list]\n",
    "            # Store the version of the enc_batch that uses the article OOV ids\n",
    "            self.enc_batch_extend_vocab = np.zeros((self.batch_size, max_enc_seq_len), dtype=np.int32)\n",
    "            for i, ex in enumerate(example_list):\n",
    "                self.enc_batch_extend_vocab[i, :] = ex.enc_inp_extend_vocab[:]\n",
    "\n",
    "    def init_decoder_seq(self, example_list):\n",
    "        # Pad the inputs and targets\n",
    "        for ex in example_list:\n",
    "            ex.pad_dec_seq(max_dec_steps, self.pad_id)\n",
    "\n",
    "        # Initialize the numpy arrays.\n",
    "        self.dec_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)\n",
    "        self.tgt_batch = np.zeros((self.batch_size, max_dec_steps), dtype=np.int32)\n",
    "        self.dec_padding_mask = np.zeros((self.batch_size, max_dec_steps), dtype=np.float32)\n",
    "        self.dec_lens = np.zeros((self.batch_size), dtype=np.int32)\n",
    "\n",
    "        # Fill in the numpy arrays\n",
    "        for i, ex in enumerate(example_list):\n",
    "            self.dec_batch[i, :] = ex.dec_inp[:]\n",
    "            self.tgt_batch[i, :] = ex.tgt[:]\n",
    "            self.dec_lens[i] = ex.dec_len\n",
    "            for j in range(ex.dec_len):\n",
    "                self.dec_padding_mask[i][j] = 1\n",
    "\n",
    "    def store_orig_strings(self, example_list):\n",
    "        self.original_articles = [ex.original_article for ex in example_list]  # list of lists\n",
    "        self.original_abstracts = [ex.original_abstract for ex in example_list]  # list of lists\n",
    "        self.original_abstracts_sents = [ex.original_abstract_sents for ex in example_list]  # list of list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = b\"the real wolf Chaky of wall has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . jordan belfort , who is played by leonardo dicaprio in the oscar-nominated film , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars of savings . ` it was about a month in when i realized . that was the first moment where i allowed greed to get the better of me , ' he told piers morgan live . scroll down for video . soulless : the real wolf of wall street , jordan belfort , pictured monday night , has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . belfort , 51 , who is now a motivational speaker , applauded the movie , and said he spent more than 100 hours with dicaprio , helping him prepare for the role of his life . he praised the actor for making the film what he calls a cautionary tale rather than glorifying the excessive behavior , as many critics have claimed the film does .\"\n",
    "abstract_sentences = [b'jordan belfort , Chaky who is played by leonardo dicaprio , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars .', b'belfort , 51 , who is now a motivational speaker , applauded the wolf of wall street , but said it was difficult to watch in parts .', b'he said he spent more than 100 hours helping dicaprio prepare for the role of his life .', b'the man who spent 22 months in prison for securities fraud and money laundering insisted to piers morgan that 95 per cent of what he did was legal .', b\"but he added ` the 5 per cent was incredibly destructive and it poisoned everything '\"]\n",
    "example1 = Example(article, abstract_sentences, vocab)\n",
    "\n",
    "article2 = b\"person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . jordan belfort , who is played by leonardo dicaprio in the oscar-nominated film , described in a monday night interview the moment he learned he was selling a scam , cheating innocent people out of millions of dollars of savings . ` it was about a month in when i realized . that was the first moment where i allowed greed to get the better of me , ' he told piers morgan live . scroll down for video . soulless : the real wolf of wall street , jordan belfort , pictured monday night , has admitted he lost his soul ` as much as a person can and still be walking around ' in the midst of his wheeling and dealing and debauched life of 90 's excess . belfort , 51 , who is now a motivational speaker , applauded the movie , and said he spent more than 100 hours with dicaprio , helping him prepare for the role of his life . he praised the actor for making the film what he calls a cautionary tale rather than glorifying the excessive behavior , as many critics have claimed the film does .\"\n",
    "abstract_sentences2 = [b'he learned he was selling a scam , cheating innocent people out of millions of dollars .', b'belfort , 51 , who is now a motivational speaker , applauded the wolf of wall street , but said it was difficult to watch in parts .', b'he said he spent more than 100 hours helping dicaprio prepare for the role of his life .', b'the man who spent 22 months in prison for securities fraud and money laundering insisted to piers morgan that 95 per cent of what he did was legal .', b\"but he added ` the 5 per cent was incredibly destructive and it poisoned everything '\"]\n",
    "example2 = Example(article, abstract_sentences, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Batch at 0x17e1f1820>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "\n",
    "b = [example1, example2]\n",
    "Batch(b, vocab, batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Batcher\n",
    "\n",
    "This is a very simple `Batcher` for providing batches of data for training, testing, etc.  It uses `Thread` and `Queue` for simultaneously reading data and putting them into the queue for high efficiency. \n",
    "\n",
    "Aside from that, it just output a batch of `(articles, abstracts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher(object):\n",
    "    BATCH_QUEUE_MAX = 100  # max number of batches the batch_queue can hold\n",
    "\n",
    "    def __init__(self, vocab, data_path, batch_size, single_pass, mode):\n",
    "        self._vocab = vocab\n",
    "        self._data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.single_pass = single_pass\n",
    "        self.mode = mode\n",
    "\n",
    "        # Initialize a queue of Batches waiting to be used, and a queue of Examples waiting to be batched\n",
    "        self._batch_queue   = queue.Queue(self.BATCH_QUEUE_MAX)\n",
    "        self._example_queue = queue.Queue(self.BATCH_QUEUE_MAX * self.batch_size)\n",
    "\n",
    "        # Different settings depending on whether we're in single_pass mode or not\n",
    "        if single_pass:\n",
    "            self._num_example_q_threads = 1  # just one thread, so we read through the dataset just once\n",
    "            self._num_batch_q_threads = 1    # just one thread to batch examples\n",
    "            self._bucketing_cache_size = 1   # only load one batch's worth of examples before bucketing\n",
    "            self._finished_reading = False   # this will tell us when we're finished reading the dataset\n",
    "        else:\n",
    "            self._num_example_q_threads = 1  # num threads to fill example queue\n",
    "            self._num_batch_q_threads = 1    # num threads to fill batch queue\n",
    "            self._bucketing_cache_size = 1   # how many batches-worth of examples to load into cache before bucketing\n",
    "\n",
    "        # Start the threads that load the queues\n",
    "        self._example_q_threads = []\n",
    "        for _ in range(self._num_example_q_threads):\n",
    "            self._example_q_threads.append(Thread(target=self.fill_example_queue))\n",
    "            self._example_q_threads[-1].daemon = True\n",
    "            self._example_q_threads[-1].start()\n",
    "        self._batch_q_threads = []\n",
    "        for _ in range(self._num_batch_q_threads):\n",
    "            self._batch_q_threads.append(Thread(target=self.fill_batch_queue))\n",
    "            self._batch_q_threads[-1].daemon = True\n",
    "            self._batch_q_threads[-1].start()\n",
    "\n",
    "        # Start a thread that watches the other threads and restarts them if they're dead\n",
    "        if not single_pass:                   # We don't want a watcher in single_pass mode because the threads shouldn't run forever\n",
    "            self._watch_thread = Thread(target=self.watch_threads)\n",
    "            self._watch_thread.daemon = True\n",
    "            self._watch_thread.start()\n",
    "\n",
    "    def next_batch(self):\n",
    "        # If the batch queue is empty, print a warning\n",
    "        if self._batch_queue.qsize() == 0:\n",
    "            logger.warning(\n",
    "                'Bucket input queue is empty when calling next_batch. Bucket queue size: %i, Input queue size: %i',\n",
    "                self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "            if self.single_pass and self._finished_reading:\n",
    "                logger.info(\"Finished reading dataset in single_pass mode.\")\n",
    "                return None\n",
    "\n",
    "        batch = self._batch_queue.get()  # get the next Batch\n",
    "        return batch\n",
    "\n",
    "    def fill_example_queue(self):\n",
    "        example_generator = self.example_generator(self._data_path, self.single_pass)\n",
    "        input_gen = self.pair_generator(example_generator)\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                (article,\n",
    "                 abstract) = input_gen.__next__()  # read the next example from file. article and abstract are both strings.\n",
    "            except StopIteration:  # if there are no more examples:\n",
    "                logger.info(\"The example generator for this example queue filling thread has exhausted data.\")\n",
    "                if self.single_pass:\n",
    "                    logger.info(\n",
    "                        \"single_pass mode is on, so we've finished reading dataset. This thread is stopping.\")\n",
    "                    self._finished_reading = True\n",
    "                    break\n",
    "                else:\n",
    "                    raise Exception(\"single_pass mode is off but the example generator is out of data; error.\")\n",
    "\n",
    "            abstract_sentences = [sent.strip() for sent in abstract2sents(\n",
    "                abstract)]  # Use the <s> and </s> tags in abstract to get a list of sentences.\n",
    "        \n",
    "            example = Example(article, abstract_sentences, self._vocab)\n",
    "            self._example_queue.put(example)\n",
    "\n",
    "    def fill_batch_queue(self):\n",
    "        while True:\n",
    "            if self.mode == 'decode':\n",
    "                # beam search decode mode single example repeated in the batch\n",
    "                ex = self._example_queue.get()\n",
    "                b = [ex for _ in range(self.batch_size)]\n",
    "                self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
    "            else:\n",
    "                # Get bucketing_cache_size-many batches of Examples into a list, then sort\n",
    "                inputs = []\n",
    "                for _ in range(self.batch_size * self._bucketing_cache_size):\n",
    "                    inputs.append(self._example_queue.get())\n",
    "                inputs = sorted(inputs, key=lambda inp: inp.enc_len, reverse=True)  # sort by length of encoder sequence\n",
    "\n",
    "                # Group the sorted Examples into batches, optionally shuffle the batches, and place in the batch queue.\n",
    "                batches = []\n",
    "                for i in range(0, len(inputs), self.batch_size):\n",
    "                    batches.append(inputs[i:i + self.batch_size])\n",
    "                if not self.single_pass:\n",
    "                    random.shuffle(batches)\n",
    "                for b in batches:  # each b is a list of Example objects\n",
    "                    self._batch_queue.put(Batch(b, self._vocab, self.batch_size))\n",
    "\n",
    "    def watch_threads(self):\n",
    "        while True:\n",
    "            logger.info(\n",
    "                'Bucket queue size: %i, Input queue size: %i',\n",
    "                self._batch_queue.qsize(), self._example_queue.qsize())\n",
    "\n",
    "            time.sleep(60)\n",
    "            for idx, t in enumerate(self._example_q_threads):\n",
    "                if not t.is_alive():  # if the thread is dead\n",
    "                    logger.error('Found example queue thread dead. Restarting.')\n",
    "                    new_t = Thread(target=self.fill_example_queue)\n",
    "                    self._example_q_threads[idx] = new_t\n",
    "                    new_t.daemon = True\n",
    "                    new_t.start()\n",
    "            for idx, t in enumerate(self._batch_q_threads):\n",
    "                if not t.is_alive():  # if the thread is dead\n",
    "                    logger.error('Found batch queue thread dead. Restarting.')\n",
    "                    new_t = Thread(target=self.fill_batch_queue)\n",
    "                    self._batch_q_threads[idx] = new_t\n",
    "                    new_t.daemon = True\n",
    "                    new_t.start()\n",
    "\n",
    "    def pair_generator(self, example_generator):\n",
    "        while True:\n",
    "            e = example_generator.__next__()  # e is a tf.Example\n",
    "            try:\n",
    "                article_text = e.features.feature['article'].bytes_list.value[\n",
    "                    0]  # the article text was saved under the key 'article' in the data files\n",
    "                abstract_text = e.features.feature['abstract'].bytes_list.value[\n",
    "                    0]  # the abstract text was saved under the key 'abstract' in the data files\n",
    "            except ValueError:\n",
    "                logger.error('Failed to get article or abstract from example')\n",
    "                continue\n",
    "            if len(article_text) == 0:  # See https://github.com/abisee/pointer-generator/issues/1\n",
    "                # logger.warning('Found an example with empty article text. Skipping it.')\n",
    "                continue\n",
    "            else:\n",
    "                yield (article_text, abstract_text)\n",
    "\n",
    "    def example_generator(self, data_path, single_pass):\n",
    "        while True:\n",
    "            filelist = glob.glob(data_path)  # get the list of datafiles\n",
    "            assert filelist, ('Error: Empty filelist at %s' % data_path)  # check filelist isn't empty\n",
    "            if single_pass:\n",
    "                filelist = sorted(filelist)\n",
    "            else:\n",
    "                random.shuffle(filelist)\n",
    "            for f in filelist:\n",
    "                reader = open(f, 'rb')\n",
    "                while True:\n",
    "                    len_bytes = reader.read(8)\n",
    "                    if not len_bytes: break  # finished reading this file\n",
    "                    str_len = struct.unpack('q', len_bytes)[0]  #q stands for integers\n",
    "                    example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                    yield example_pb2.Example.FromString(example_str)\n",
    "            if single_pass:\n",
    "                print(\"example_generator completed reading all datafiles. No more data.\")\n",
    "                break\n",
    "            \n",
    "### utility functions\n",
    "    \n",
    "def abstract2sents(abstract):\n",
    "    cur_p = 0\n",
    "    sents = []\n",
    "    while True:\n",
    "        try:\n",
    "            sta_p = abstract.index(SENTENCE_STA.encode(), cur_p)\n",
    "            end_p = abstract.index(SENTENCE_END.encode(), sta_p + 1)\n",
    "            cur_p = end_p + len(SENTENCE_END.encode())\n",
    "            sents.append(abstract[sta_p + len(SENTENCE_STA.encode()):end_p])\n",
    "        except ValueError as e:  # no more sentences\n",
    "            return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcher = Batcher(vocab, train_data_path, batch_size, single_pass=False, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batcher.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([400, 400, 400], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_lens = batch.enc_lens\n",
    "enc_lens  #(length of first batch, length of second batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 400])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_batch = torch.from_numpy(batch.enc_batch).long()\n",
    "enc_batch.shape  #(batch size, seq len)  #seq len is padded to max length thus same length for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 400])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_padding_mask = torch.from_numpy(batch.enc_padding_mask).float()\n",
    "enc_padding_mask.shape #(batch size, seq len)  #simply indicates which position is real word or padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 400])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if pointer_gen:\n",
    "enc_batch_extend_vocab = torch.from_numpy(batch.enc_batch_extend_vocab).long()\n",
    "enc_batch_extend_vocab.shape  #(batch size, seq len) #used if we use pointer_gen, which has dedicated ID of oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 18])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch.max_art_oovs > 0:\n",
    "#this will be used to append to the vocab distribution, since we will have some OOV vocab\n",
    "extra_zeros = torch.zeros((batch_size, batch.max_art_oovs))\n",
    "extra_zeros.shape  #(batch_size, max number of in-article OOVs in this batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model\n",
    "\n",
    "<img src = \"images/gettothepoint.png\" width=800>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Basic Module\n",
    "\n",
    "The author implements truncated normal distribution to initialize the parameters.  To impose this initialization on all module, they created an abstract class called BasicModule (which then other module will inherits), which implements the truncated normal distribution.\n",
    "\n",
    "As to why truncated is good is likely related to better activation.  Read this: https://stats.stackexchange.com/questions/228670/what-is-the-benefit-of-the-truncated-normal-distribution-in-initializing-weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModule(nn.Module):\n",
    "    def __init__(self, init='uniform'):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.init = init\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if self.init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif self.init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif self.init == 'truncated_normal':\n",
    "                    self.truncated_normal_(param, mean=0,std=stddev)\n",
    "\n",
    "    def truncated_normal_(self, tensor, mean=0, std=1.):\n",
    "        \"\"\"\n",
    "        Implemented by @ruotianluo\n",
    "        See https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/15\n",
    "        \"\"\"\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the truncated normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal_(tensor, mean=0, std=1):\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "    return tensor\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "\n",
    "def test_truncnorm():\n",
    "    a, b = -2, 2\n",
    "    size = 1000000\n",
    "    r = truncnorm.rvs(a, b, size=size)\n",
    "    ax.hist(r, density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "\n",
    "    tensor = torch.zeros(size)\n",
    "    truncated_normal_(tensor)\n",
    "    r = tensor.numpy()\n",
    "\n",
    "    ax.hist(r, density=True, histtype='stepfilled', alpha=0.2, bins=50)\n",
    "    ax.legend(loc='best', frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_truncnorm()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attention\n",
    "\n",
    "Attention used in this paper is of additive attention.  Don't worry, it is simply equation of:\n",
    "\n",
    "$$e^t_i = v^T\\text{tanh}(W_hh_i + W_ss_t + b_{\\text{attn}})$$\n",
    "$$a^t = \\text{softmax}(e^t)$$\n",
    "\n",
    "The paper also introduced a coverage mechanism. A coverage vector $c^t$ is maintained which is the sum of attention distributions over all previous decoder timesteps.  \n",
    "\n",
    "$$c^t = \\sum^{t-1}_{t'=0}a^{t'}$$\n",
    "\n",
    "Intuitively, it is a (unnormalized) distribution over the source document words that represents the degree of coverage that those words have received from the attention mechanism so far.\n",
    "\n",
    "The coverage vector is used as extra input to the attention mechanism, changing the equation to\n",
    "\n",
    "$$e^t_i = v^T\\text{tanh}(W_hh_i + W_ss_t + w_cc^t_i + b_{\\text{attn}})$$\n",
    "\n",
    "This ensures that the attention mechanism’s current decision (choosing where to attend next) is informed by a reminder of its previous decisions (summarized in $c^t$). This should make it easier for the attention mechanism to avoid repeatedly attending to the same locations, and thus avoid generating repetitive text.\n",
    "\n",
    "In addition, the author define a coverage loss to penalize repeatedly attending to the same locations:\n",
    "\n",
    "$$\\text{covloss}_t = \\sum_i \\text{min}(a_i^t, c_i^t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(BasicModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
    "        self.dec_fc = nn.Linear(hidden_dim * 2, hidden_dim * 2)  #<---for st\n",
    "        self.enc_out_fc = nn.Linear(hidden_dim * 2, hidden_dim * 2, bias=False) #<---for hi\n",
    "        if is_coverage:\n",
    "            self.con_fc = nn.Linear(1, hidden_dim * 2, bias=False)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, s_t, enc_out, enc_padding_mask, coverage):\n",
    "        \n",
    "        #s_t:              [batch size, hid dim * 2]  ==> concat of h_t and c_t\n",
    "        #enc_out:          [batch size, seq len, hid dim * 2] ==> all hidden states \n",
    "        #enc_padding_mask: [batch size, seq len]\n",
    "        #coverage:         [batch_size, seq len]\n",
    "                \n",
    "        b, l, n = list(enc_out.size())  #batch size, seq len, hid dim * 2\n",
    "        enc_out = enc_out.contiguous()  #contingous so we can view....view is more efficient than reshape\n",
    "        enc_fea = enc_out.view(-1, 2 * hidden_dim)\n",
    "        #encoder_feature: [batch size * seq len, hidden dim*directions]\n",
    "        \n",
    "        enc_fea = self.enc_out_fc(enc_fea)  #<---this is the Wh @ hi part in the paper\n",
    "        #encoder_feature: [batch size * seq len, hidden dim*directions]\n",
    "\n",
    "        dec_fea = self.dec_fc(s_t)  #<----this is the Ws @ st part in the paper\n",
    "        # dec_fea: [batch size, hid dim * 2]        \n",
    "        \n",
    "        dec_fea_expanded = dec_fea.unsqueeze(1).expand(b, l, n).contiguous()  # dec_fea_expanded: [batch size, seq len, hid dim * 2]\n",
    "        #imagine we copy this seq_len times, so the st can be used seq_len times with each encoder hidden states\n",
    "        dec_fea_expanded = dec_fea_expanded.view(-1, n)     \n",
    "        # dec_fea_expanded: [batch size * seq len, hid dim * 2]\n",
    "\n",
    "        #add \n",
    "        att_features = enc_fea + dec_fea_expanded  #note that b is already included in dec_fc         \n",
    "        # att_features: [batch size * seq len, hid dim * 2]\n",
    "        \n",
    "        #attention is equal to previous attention + current attention\n",
    "        if is_coverage:\n",
    "            coverage_inp = coverage.view(-1, 1)             # coverage_inp:     [batch size * seq len, 1]\n",
    "            coverage_fea = self.con_fc(coverage_inp)        # coverage_fea:     [batch size * seq len, hid dim * 2]\n",
    "            att_features = att_features + coverage_fea      # att_features:     [batch size * seq len, hid dim * 2]\n",
    "\n",
    "        e = torch.tanh(att_features)                        \n",
    "        # e:          [batch size * seq len, hid dim * 2]\n",
    "        scores = self.fc(e)                                 \n",
    "        # scores:     [batch size * seq len, 1]\n",
    "        scores = scores.view(-1, l)                         \n",
    "        # scores:     [batch size, seq len]\n",
    "\n",
    "        attn_dist_ = F.softmax(scores, dim=1) * enc_padding_mask  \n",
    "        # attn_dist_:     [batch size, seq len]\n",
    "        normalization_factor = attn_dist_.sum(1, keepdim=True)\n",
    "        attn_dist = attn_dist_ / normalization_factor             #simply devide by the sum\n",
    "\n",
    "        attn_dist = attn_dist.unsqueeze(1)                        \n",
    "        # attn_dist_:     [batch size, 1, seq len]\n",
    "        c_t = torch.bmm(attn_dist, enc_out)                       \n",
    "        # c_t:            [batch size, 1, hid dim * 2]\n",
    "        c_t = c_t.view(-1, hidden_dim * 2)                        \n",
    "        # c_t:            [batch size, hid dim * 2]\n",
    "\n",
    "        attn_dist = attn_dist.view(-1, l)                         \n",
    "        # attn_dist:      [batch size, seq len]\n",
    "\n",
    "        #accumulate attention into coverage.....\n",
    "        if is_coverage:\n",
    "            coverage = coverage.view(-1, l)                       \n",
    "            # coverage:        [batch_size, seq len]\n",
    "            coverage = coverage + attn_dist                       \n",
    "            # coverage:        [batch_size, seq len]\n",
    "\n",
    "        return c_t, attn_dist, coverage\n",
    "        # c_t:            [batch size, 1, hid dim * 2]\n",
    "        # attn_dist:      [batch size, seq len]\n",
    "        # coverage:       [batch_size, seq len]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Encoder\n",
    "\n",
    "A very simple bidrectional LSTM-based encoder, which encodes data using LSTM follow by a simple linear layer that output a hidden state.  It uses `pack_padded_sequence` for efficiency and to ignore paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.init_params()\n",
    "\n",
    "    # seq_lens should be in descending order\n",
    "    def forward(self, input, seq_lens):\n",
    "        #input: [batch_size, seq len]\n",
    "        \n",
    "        embedded = self.src_word_emb(input)  \n",
    "        #embedded: [batch_size, seq len, emb dim]\n",
    "\n",
    "        packed = pack_padded_sequence(embedded, seq_lens, batch_first=True)  #efficient sparse matrix, also ignoring padding\n",
    "        output, hidden = self.lstm(packed)\n",
    "        #hidden = h, c\n",
    "        #h: [directions*num_layers, batch size, hidden_dim]\n",
    "        #c: [directions*num_layers, batch size, hidden_dim]\n",
    "\n",
    "        encoder_outputs, _ = pad_packed_sequence(output, batch_first=True)\n",
    "        #encoder_outputs: [batch size, seq len, hidden dim*directions]\n",
    "\n",
    "        return encoder_outputs, hidden\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reduced State\n",
    "\n",
    "Just a very simple pipe, which reduce the hidden and cell states from `hidden_dim*2` to `hidden_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceState(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(ReduceState, self).__init__()\n",
    "\n",
    "        self.reduce_h = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.reduce_c = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        h, c = hidden\n",
    "        #h, c: [directions*num_layers, batch size, hidden_dim]\n",
    "\n",
    "        h_in = h.transpose(0, 1).contiguous().view(-1, hidden_dim * 2)\n",
    "        #h_in: [batch_size, hidden_dim*direction*num_layers]\n",
    "        \n",
    "        hidden_reduced_h = F.relu(self.reduce_h(h_in))\n",
    "        #hidden_reduced_h: [batch_size, hidden_dim]\n",
    "        \n",
    "        c_in = c.transpose(0, 1).contiguous().view(-1, hidden_dim * 2)\n",
    "        #c_in: [batch_size, hidden_dim*direction*num_layers]\n",
    "        \n",
    "        hidden_reduced_c = F.relu(self.reduce_c(c_in))\n",
    "        #hidden_reduced_c: [batch_size, hidden_dim]\n",
    "\n",
    "        return (hidden_reduced_h.unsqueeze(0), hidden_reduced_c.unsqueeze(0))  # [1, batch_size, hidden_dim]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Decoder\n",
    "\n",
    "Here, pointer mechanism is used.  I am too lazy, so I just captured the screenshot:\n",
    "\n",
    "<img src = \"images/pointer.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        #attention\n",
    "        self.attention_network = Attention()\n",
    "        \n",
    "        # decoder\n",
    "        self.tgt_word_emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.con_fc = nn.Linear(hidden_dim * 2 + emb_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "\n",
    "        if pointer_gen:\n",
    "            self.p_gen_fc = nn.Linear(hidden_dim * 4 + emb_dim, 1)\n",
    "\n",
    "        # p_vocab\n",
    "        self.fc1 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, y_t, s_t, enc_out, enc_padding_mask,\n",
    "                c_t, extra_zeros, enc_batch_extend_vocab, coverage, step):\n",
    "        \n",
    "        #y_t:              [batch size]\n",
    "        #s_t ==> h, c:     [1, batch size, hid dim]  ==> reduced last hidden states from encoder\n",
    "        #enc_out:          [batch size, seq len, hid dim * 2]  ==> all hidden states \n",
    "        #enc_padding_mask: [batch size, seq len]\n",
    "        #c_t:              [batch_size, hid dim * 2] ==> context vector\n",
    "        #extra_zeros:      [batch_size, batch.max_art_oovs] ==> max number of in-article OOVs in this batch\n",
    "        #enc_batch_extend_vocab: [batch size, seq len] ==> used if we use pointer_gen, which has dedicated ID of oov\n",
    "        #coverage:         [batch_size, seq len] ==> accumulation fo attention\n",
    "        \n",
    "        if step == 0:\n",
    "            dec_h, dec_c = s_t\n",
    "            s_t_hat = torch.cat((dec_h.view(-1, hidden_dim),\n",
    "                                 dec_c.view(-1, hidden_dim)), 1)\n",
    "            #s_t_hat: [batch size, hid dim * 2]\n",
    "            c_t, _, coverage_next = self.attention_network(s_t_hat, enc_out,\n",
    "                                                           enc_padding_mask, coverage)\n",
    "            # c_t:            [batch size, 1, hid dim * 2] ==>context vector\n",
    "            # attn_dist:      [batch size, seq len] ==> attention\n",
    "            # coverage:       [batch_size, seq len] ==> accumulation of attention\n",
    "            coverage = coverage_next\n",
    "\n",
    "        y_t_embd = self.tgt_word_emb(y_t)\n",
    "        #y_t_embd: [batch size, emb_dim]\n",
    "        #concat the context vector with current token and send to linear layer\n",
    "        x = self.con_fc(torch.cat((c_t, y_t_embd), 1))\n",
    "        #x: [batch size, emb_dim]\n",
    "        #then send this to the lstm cell for decoding\n",
    "        lstm_out, s_t = self.lstm(x.unsqueeze(1), s_t)  #note that s_t is the hidden state which comes from previous\n",
    "        #lstm_out: [batch size, 1, hid_dim]\n",
    "        \n",
    "        dec_h, dec_c = s_t\n",
    "        #dec_h = dec_c: [1, batch size, hid_dim]\n",
    "        \n",
    "        #combine hidden and cell state, to be used for next decoding as well as calculating attention with encoder outputs\n",
    "        s_t_hat = torch.cat((dec_h.view(-1, hidden_dim),\n",
    "                             dec_c.view(-1, hidden_dim)), 1)  \n",
    "        #s_t_hat: [batch size, hid_dim * 2]\n",
    "        \n",
    "        #calculating the attention to get the context vector for next decoding step\n",
    "        c_t, attn_dist, coverage_next = self.attention_network(s_t_hat, enc_out,\n",
    "                                                               enc_padding_mask, coverage)\n",
    "        #c_t: [batch size, hid_dim * 2]\n",
    "        #attn_dist: [batch size, enc_seq_len]\n",
    "        #coverage_next = [batch size, enc_seq_len]\n",
    "\n",
    "        if step > 0:\n",
    "            coverage = coverage_next\n",
    "\n",
    "        p_gen = None\n",
    "        if pointer_gen:\n",
    "            p_gen_inp = torch.cat((c_t, s_t_hat, x), 1)  # B x (2*2*hidden_dim + emb_dim)\n",
    "            print(f\"{p_gen_inp.shape=}\")\n",
    "            #p_gen_inp: [batch size, 2 * 2 * hid_dim + emb_dim], i.e., 1152\n",
    "\n",
    "            p_gen = self.p_gen_fc(p_gen_inp)\n",
    "            print(f\"{p_gen.shape=}\")\n",
    "\n",
    "            p_gen = torch.sigmoid(p_gen)\n",
    "            print(f\"{p_gen.shape=}\")\n",
    "            \n",
    "                '''\n",
    "        p_gen.shape=torch.Size([3, 1])\n",
    "        p_gen.shape=torch.Size([3, 1])\n",
    "        output.shape=torch.Size([3, 768])\n",
    "        output.shape=torch.Size([3, 256])\n",
    "        output.shape=torch.Size([3, 50000])\n",
    "        vocab_dist.shape=torch.Size([3, 50000])\n",
    "        final_dist.shape=torch.Size([3, 50018])\n",
    "        tensor([[ 446],\n",
    "                [1953],\n",
    "                [4621]])\n",
    "        torch.Size([3, 50018])\n",
    "        '''\n",
    "\n",
    "\n",
    "        output = torch.cat((lstm_out.view(-1, hidden_dim), c_t), 1)  # B x hidden_dim * 3\n",
    "        \n",
    "        print(f\"{output.shape=}\")\n",
    "\n",
    "        \n",
    "        output = self.fc1(output)  # B x hidden_dim\n",
    "        print(f\"{output.shape=}\")\n",
    "\n",
    "        # output = F.relu(output)\n",
    "\n",
    "        output = self.fc2(output)  # B x vocab_size\n",
    "        print(f\"{output.shape=}\")\n",
    "\n",
    "        vocab_dist = F.softmax(output, dim=1)\n",
    "        print(f\"{vocab_dist.shape=}\")\n",
    "\n",
    "\n",
    "        if pointer_gen:\n",
    "            vocab_dist_ = p_gen * vocab_dist\n",
    "            attn_dist_ = (1 - p_gen) * attn_dist\n",
    "\n",
    "            if extra_zeros is not None:\n",
    "                vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], 1)\n",
    "\n",
    "            final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
    "            print(f\"{final_dist.shape=}\")\n",
    "\n",
    "        else:\n",
    "            final_dist = vocab_dist\n",
    "\n",
    "        return final_dist, s_t, c_t, attn_dist, p_gen, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, model_path=None):\n",
    "        encoder = Encoder()\n",
    "        decoder = Decoder()\n",
    "        reduce_state = ReduceState()\n",
    "\n",
    "        # shared the embedding between encoder and decoder\n",
    "        decoder.tgt_word_emb.weight = encoder.src_word_emb.weight\n",
    "\n",
    "        encoder = encoder.to(device)\n",
    "        decoder = decoder.to(device)\n",
    "        reduce_state = reduce_state.to(device)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reduce_state = reduce_state\n",
    "\n",
    "        if model_path is not None:\n",
    "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
    "            self.encoder.load_state_dict(state['encoder_state_dict'])\n",
    "            self.decoder.load_state_dict(state['decoder_state_dict'], strict=False)\n",
    "            self.reduce_state.load_state_dict(state['reduce_state_dict'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define since above...\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 400]), (3,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is from where we test the Batcher....\n",
    "enc_batch.shape, enc_lens.shape #[batch size, seq len], [batch size]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 400, 512]), torch.Size([2, 3, 256]), torch.Size([2, 3, 256]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. encoding inputs\n",
    "chaky_model = Model()\n",
    "enc_out, enc_h = chaky_model.encoder(enc_batch, enc_lens)\n",
    "h, c = enc_h  \n",
    "enc_out.shape, h.shape, c.shape #[batch size, seq len, hid dim * 2], [batch size * seq len, hid dim * 2], [num dir, batch size, hid dim], [num dir, batch size, hid dim]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Reduced state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 256]), torch.Size([1, 3, 256]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. reduced states from hidden_dim * 2 to hidden_dim\n",
    "s_t = chaky_model.reduce_state(enc_h)\n",
    "reduced_h, reduced_c = s_t\n",
    "reduced_h.shape, reduced_c.shape #[1, batch size, hid dim], [1, batch size, hid dim]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_lens=array([60, 34, 57], dtype=int32)\n",
      "dec_batch[0]=tensor([    2,   446,   147,  1037,  1176,   290, 10001,    11,     8,  2628,\n",
      "         3937,    11,  1031, 32988,     4, 19119, 13625,     9, 14725, 11678,\n",
      "          191,  1063,    11,     5,   221,   439,     7,   377,   147,   879,\n",
      "            4,  2381,  5209,   112,  4743,   360,   435,    10,     5,  2628,\n",
      "           96,   100,  6816,     4,  4235,  5122,    12,   290,    30,    48,\n",
      "         2491,  1022,   597,    11,     5,   869,   581,    45,   153,     4,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "tgt_batch[0]=tensor([  446,   147,  1037,  1176,   290, 10001,    11,     8,  2628,  3937,\n",
      "           11,  1031, 32988,     4, 19119, 13625,     9, 14725, 11678,   191,\n",
      "         1063,    11,     5,   221,   439,     7,   377,   147,   879,     4,\n",
      "         2381,  5209,   112,  4743,   360,   435,    10,     5,  2628,    96,\n",
      "          100,  6816,     4,  4235,  5122,    12,   290,    30,    48,  2491,\n",
      "         1022,   597,    11,     5,   869,   581,    45,   153,     4,     3,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "dec_padding_mask[0]=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "dec_pos=tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "         55, 56, 57, 58, 59, 60,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
      "         55, 56, 57,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "#3. decoding\n",
    "\n",
    "#get length of dec\n",
    "dec_lens = batch.dec_lens  #[batch size]\n",
    "print(f\"{dec_lens=}\")\n",
    "max_dec_len = np.max(dec_lens)\n",
    "dec_lens = torch.from_numpy(dec_lens).float()  #convert to tensor\n",
    "batch_size = len(batch.dec_lens)\n",
    "\n",
    "#dec_batch\n",
    "dec_batch = torch.from_numpy(batch.dec_batch).long()\n",
    "print(f\"{dec_batch[0]=}\") #[batch size, max dec len]\n",
    "\n",
    "#tgt_batch - one word ahead of dec_batch\n",
    "tgt_batch = torch.from_numpy(batch.tgt_batch).long()\n",
    "print(f\"{tgt_batch[0]=}\") #[batch size, max dec len]\n",
    "\n",
    "#dec_padding_mask: 0 to indicate that pos is padding\n",
    "dec_padding_mask = torch.from_numpy(batch.dec_padding_mask).float()\n",
    "print(f\"{dec_padding_mask[0]=}\") #[batch size, max dec len]\n",
    "\n",
    "#dec_pos: a running number 1, 2, 3 until padding\n",
    "dec_pos = np.zeros((batch_size, max_dec_steps)) #[batch size, max dec steps]\n",
    "\n",
    "for i, inst in enumerate(batch.dec_batch):\n",
    "    for j, w_i in enumerate(inst):\n",
    "        if w_i != PAD:\n",
    "            dec_pos[i, j] = (j + 1)\n",
    "        else:\n",
    "            break\n",
    "dec_pos = torch.from_numpy(dec_pos).long()\n",
    "print(f\"{dec_pos=}\") #[batch size, max dec len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 100\n",
      "y_t_embd.shape=torch.Size([3, 128])\n",
      "x.shape=torch.Size([3, 128])\n",
      "lstm_out.shape=torch.Size([3, 1, 256])\n",
      "dec_h.shape=torch.Size([1, 3, 256])\n",
      "s_t_hat.shape=torch.Size([3, 512])\n",
      "c_t.shape=torch.Size([3, 512])\n",
      "attn_dist.shape=torch.Size([3, 400])\n",
      "coverage_next.shape=torch.Size([3, 400])\n",
      "p_gen_inp.shape=torch.Size([3, 1152])\n",
      "p_gen.shape=torch.Size([3, 1])\n",
      "p_gen.shape=torch.Size([3, 1])\n",
      "output.shape=torch.Size([3, 768])\n",
      "output.shape=torch.Size([3, 256])\n",
      "output.shape=torch.Size([3, 50000])\n",
      "vocab_dist.shape=torch.Size([3, 50000])\n",
      "final_dist.shape=torch.Size([3, 50018])\n",
      "tensor([[ 446],\n",
      "        [1953],\n",
      "        [4621]])\n",
      "torch.Size([3, 50018])\n"
     ]
    }
   ],
   "source": [
    "print(max_dec_len, max_dec_steps) #max length of all batch sentences, max tokens our decoding will work on\n",
    "for di in range(min(max_dec_len, max_dec_steps)):\n",
    "    y_t = dec_batch[:, di]  #the input to the decoder, one by one, and we plan to output the next token\n",
    "    c_t = torch.zeros((batch_size, 2 * hidden_dim))   #simply the first c_t, like c_t0 which is randomized....\n",
    "    coverage = torch.zeros(enc_batch.size()) #[batch size, ]\n",
    "    \n",
    "    final_dist, s_t, c_t, attn_dist, p_gen, next_coverage = \\\n",
    "    chaky_model.decoder(y_t, s_t, enc_out, enc_padding_mask, c_t,\n",
    "                        extra_zeros, enc_batch_extend_vocab, coverage, di)\n",
    "    \n",
    "    tgt = tgt_batch[:, di]  #[batch size, ]\n",
    "    step_mask = dec_padding_mask[:, di] #[batch size, ]\n",
    "    \n",
    "    print(tgt.unsqueeze(1))\n",
    "    \n",
    "    print(final_dist.shape)\n",
    "\n",
    "    gold_probs = torch.gather(final_dist, 1, tgt.unsqueeze(1)).squeeze()\n",
    "    step_loss = -torch.log(gold_probs + eps)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4067800170.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[42], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    stop here\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            if is_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + cov_loss_wt * step_coverage_loss\n",
    "                cove_losses.append(step_coverage_loss * step_mask)\n",
    "                coverage = next_coverage\n",
    "\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses / dec_lens\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self):\n",
    "        self.vocab = Vocab(vocab_path, vocab_size)\n",
    "        self.batcher = Batcher(self.vocab, train_data_path,\n",
    "                               batch_size, single_pass=False, mode='train')\n",
    "        time.sleep(10)\n",
    "\n",
    "        train_dir = os.path.join(log_root, 'train_%d' % (int(time.time())))\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.mkdir(train_dir)\n",
    "\n",
    "        self.model_dir = os.path.join(train_dir, 'models')\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "            \n",
    "    def save_model(self, running_avg_loss, iter):\n",
    "        state = {\n",
    "            'iter': iter,\n",
    "            'encoder_state_dict': self.model.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.model.decoder.state_dict(),\n",
    "            'reduce_state_dict': self.model.reduce_state.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'current_loss': running_avg_loss\n",
    "        }\n",
    "        model_save_path = os.path.join(self.model_dir, 'model_%d_%d' % (iter, int(time.time())))\n",
    "        torch.save(state, model_save_path)\n",
    "\n",
    "    def setup_train(self, model_path=None):\n",
    "        self.model = Model(model_path)\n",
    "        initial_lr = lr_coverage if is_coverage else lr\n",
    "\n",
    "        params = list(self.model.encoder.parameters()) + list(self.model.decoder.parameters()) + \\\n",
    "                 list(self.model.reduce_state.parameters())\n",
    "        total_params = sum([param[0].nelement() for param in params])\n",
    "        print('The Number of params of model: %.3f million' % (total_params / 1e6))  # million\n",
    "        self.optimizer = optim.Adagrad(params, lr=initial_lr, initial_accumulator_value=adagrad_init_acc)\n",
    "\n",
    "        start_iter, start_loss = 0, 0\n",
    "\n",
    "        if model_path is not None:\n",
    "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
    "            start_iter = state['iter']\n",
    "            start_loss = state['current_loss']\n",
    "\n",
    "            if not is_coverage:\n",
    "                self.optimizer.load_state_dict(state['optimizer'])\n",
    "                for state in self.optimizer.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if torch.is_tensor(v):\n",
    "                            state[k] = v.to(device)\n",
    "\n",
    "        return start_iter, start_loss\n",
    "\n",
    "    def train_one_batch(self, batch):\n",
    "        enc_batch, enc_lens, enc_pos, enc_padding_mask, enc_batch_extend_vocab, \\\n",
    "        extra_zeros, c_t, coverage = get_input_from_batch(batch)\n",
    "        dec_batch, dec_lens, dec_pos, dec_padding_mask, max_dec_len, tgt_batch = \\\n",
    "            get_output_from_batch(batch)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_lens)\n",
    "        \n",
    "        s_t = self.model.reduce_state(enc_h)\n",
    "\n",
    "        step_losses, cove_losses = [], []\n",
    "        for di in range(min(max_dec_len, max_dec_steps)):\n",
    "            y_t = dec_batch[:, di]  # Teacher forcing\n",
    "            final_dist, s_t, c_t, attn_dist, p_gen, next_coverage = \\\n",
    "                self.model.decoder(y_t, s_t, enc_out, enc_fea, enc_padding_mask, c_t,\n",
    "                                   extra_zeros, enc_batch_extend_vocab, coverage, di)\n",
    "            tgt = tgt_batch[:, di]\n",
    "            step_mask = dec_padding_mask[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, tgt.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs + eps)\n",
    "            if is_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + cov_loss_wt * step_coverage_loss\n",
    "                cove_losses.append(step_coverage_loss * step_mask)\n",
    "                coverage = next_coverage\n",
    "\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses / dec_lens\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(self.model.encoder.parameters(), max_grad_norm)\n",
    "        clip_grad_norm_(self.model.decoder.parameters(), max_grad_norm)\n",
    "        clip_grad_norm_(self.model.reduce_state.parameters(), max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if is_coverage:\n",
    "            cove_losses = torch.sum(torch.stack(cove_losses, 1), 1)\n",
    "            batch_cove_loss = cove_losses / dec_lens\n",
    "            batch_cove_loss = torch.mean(batch_cove_loss)\n",
    "            return loss.item(), batch_cove_loss.item()\n",
    "\n",
    "        return loss.item(), 0.\n",
    "\n",
    "    def run(self, n_iters, model_path=None):\n",
    "        iter, running_avg_loss = self.setup_train(model_path)\n",
    "        start = time.time()\n",
    "        interval = 100\n",
    "\n",
    "        while iter < n_iters:\n",
    "            batch = self.batcher.next_batch()\n",
    "            loss, cove_loss = self.train_one_batch(batch)\n",
    "\n",
    "            running_avg_loss = calc_running_avg_loss(loss, running_avg_loss, iter)\n",
    "            iter += 1\n",
    "\n",
    "            if iter % interval == 0:\n",
    "                print(\n",
    "                    'step: %d, second: %.2f , loss: %f, cover_loss: %f' % (iter, time.time() - start, loss, cove_loss))\n",
    "                start = time.time()\n",
    "            if iter % 5000 == 0:\n",
    "                self.save_model(running_avg_loss, iter)\n",
    "                \n",
    "    \n",
    "def calc_running_avg_loss(loss, running_avg_loss, step, decay=0.99):\n",
    "    if running_avg_loss == 0:  # on the first iteration just take the loss\n",
    "        running_avg_loss = loss\n",
    "    else:\n",
    "        running_avg_loss = running_avg_loss * decay + (1 - decay) * loss\n",
    "    running_avg_loss = min(running_avg_loss, 12)  # clip\n",
    "    \n",
    "    return running_avg_loss\n",
    "\n",
    "def get_output_from_batch(batch):\n",
    "    dec_lens = batch.dec_lens\n",
    "    max_dec_len = np.max(dec_lens)\n",
    "    batch_size = len(batch.dec_lens)\n",
    "    dec_lens  = torch.from_numpy(dec_lens).float()\n",
    "    tgt_batch = torch.from_numpy(batch.tgt_batch).long()\n",
    "    dec_batch = torch.from_numpy(batch.dec_batch).long()\n",
    "    dec_padding_mask = torch.from_numpy(batch.dec_padding_mask).float()\n",
    "\n",
    "    dec_pos = np.zeros((batch_size, max_dec_steps))\n",
    "    for i, inst in enumerate(batch.dec_batch):\n",
    "        for j, w_i in enumerate(inst):\n",
    "            if w_i != PAD:\n",
    "                dec_pos[i, j] = (j + 1)\n",
    "            else:\n",
    "                break\n",
    "    dec_pos = torch.from_numpy(dec_pos).long()\n",
    "\n",
    "    dec_lens = dec_lens.to(device)\n",
    "    tgt_batch = tgt_batch.to(device)\n",
    "    dec_batch = dec_batch.to(device)\n",
    "    dec_padding_mask = dec_padding_mask.to(device)\n",
    "    dec_pos = dec_pos.to(device)\n",
    "\n",
    "    return dec_batch, dec_lens, dec_pos, dec_padding_mask, max_dec_len, tgt_batch\n",
    "\n",
    "####utility functions for Train()\n",
    "\n",
    "def get_input_from_batch(batch):\n",
    "    extra_zeros = None\n",
    "    enc_lens = batch.enc_lens\n",
    "    max_enc_len = np.max(enc_lens)\n",
    "    enc_batch_extend_vocab = None\n",
    "    batch_size = len(batch.enc_lens)\n",
    "    enc_batch = torch.from_numpy(batch.enc_batch).long()\n",
    "    enc_padding_mask = torch.from_numpy(batch.enc_padding_mask).float()\n",
    "\n",
    "    if pointer_gen:\n",
    "        enc_batch_extend_vocab = torch.from_numpy(batch.enc_batch_extend_vocab).long()\n",
    "        # max_art_oovs is the max over all the article oov list in the batch\n",
    "        if batch.max_art_oovs > 0:\n",
    "            extra_zeros = torch.zeros((batch_size, batch.max_art_oovs))\n",
    "\n",
    "    c_t = torch.zeros((batch_size, 2 * hidden_dim))\n",
    "\n",
    "    coverage = None\n",
    "    if is_coverage:\n",
    "        coverage = torch.zeros(enc_batch.size())\n",
    "\n",
    "    enc_pos = np.zeros((batch_size, max_enc_len))\n",
    "    for i, inst in enumerate(batch.enc_batch):\n",
    "        for j, w_i in enumerate(inst):\n",
    "            if w_i != PAD:\n",
    "                enc_pos[i, j] = (j + 1)\n",
    "            else:\n",
    "                break\n",
    "    enc_pos = torch.from_numpy(enc_pos).long()\n",
    "\n",
    "    c_t = c_t.to(device)\n",
    "    enc_pos = enc_pos.to(device)\n",
    "    enc_batch = enc_batch.to(device)\n",
    "    enc_padding_mask = enc_padding_mask.to(device)\n",
    "\n",
    "    if coverage is not None:\n",
    "        coverage = coverage.to(device)\n",
    "\n",
    "    if extra_zeros is not None:\n",
    "        extra_zeros = extra_zeros.to(device)\n",
    "\n",
    "    if enc_batch_extend_vocab is not None:\n",
    "        enc_batch_extend_vocab = enc_batch_extend_vocab.to(device)\n",
    "\n",
    "    return enc_batch, enc_lens, enc_pos, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, c_t, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run(max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(object):\n",
    "    def __init__(self, model_path):\n",
    "        self.vocab = Vocab(vocab_path, vocab_size)\n",
    "        self.batcher = Batcher(eval_data_path, self.vocab, mode='eval',\n",
    "                               batch_size=batch_size, single_pass=True)\n",
    "        time.sleep(15)\n",
    "        model_name = os.path.basename(model_path)\n",
    "\n",
    "        eval_dir = os.path.join(log_root, 'eval_%s' % (model_name))\n",
    "        if not os.path.exists(eval_dir):\n",
    "            os.mkdir(eval_dir)\n",
    "\n",
    "        self.model = Model(model_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def eval_one_batch(self, batch):\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t, coverage = \\\n",
    "            get_input_from_batch(batch)\n",
    "        dec_batch, dec_padding_mask, max_dec_len, dec_lens_var, tgt_batch = \\\n",
    "            get_output_from_batch(batch)\n",
    "\n",
    "        enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_lens)\n",
    "        s_t = self.model.reduce_state(enc_h)\n",
    "\n",
    "        step_losses = []\n",
    "        for di in range(min(max_dec_len, max_dec_steps)):\n",
    "            y_t = dec_batch[:, di]  # Teacher forcing\n",
    "            final_dist, s_t, c_t,attn_dist, p_gen, next_coverage = self.model.decoder(y_t, s_t,\n",
    "                                                        enc_out, enc_fea, enc_padding_mask, c_t,\n",
    "                                                        extra_zeros, enc_batch_extend_vocab, coverage, di)\n",
    "            tgt = tgt_batch[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, tgt.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs + eps)\n",
    "            if is_coverage:\n",
    "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
    "                step_loss = step_loss + cov_loss_wt * step_coverage_loss\n",
    "                coverage = next_coverage\n",
    "\n",
    "            step_mask = dec_padding_mask[:, di]\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "        sum_step_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_step_losses / dec_lens_var\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        return loss.data[0]\n",
    "\n",
    "    def run(self):\n",
    "        start = time.time()\n",
    "        running_avg_loss, iter = 0, 0\n",
    "        batch = self.batcher.next_batch()\n",
    "        print_interval = 100\n",
    "        while batch is not None:\n",
    "            loss = self.eval_one_batch(batch)\n",
    "            running_avg_loss = calc_running_avg_loss(loss, running_avg_loss, iter)\n",
    "            iter += 1\n",
    "\n",
    "            if iter % print_interval == 0:\n",
    "                print('step: %d, second: %.2f , loss: %f' % (iter, time.time() - start, running_avg_loss))\n",
    "                start = time.time()\n",
    "            batch = self.batcher.next_batch()\n",
    "\n",
    "        return running_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_processor = Evaluate(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_processor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam(object):\n",
    "    def __init__(self, tokens, log_probs, state, context, coverage):\n",
    "        self.tokens = tokens\n",
    "        self.state = state\n",
    "        self.context = context\n",
    "        self.coverage = coverage\n",
    "        self.log_probs = log_probs\n",
    "\n",
    "    def extend(self, token, log_prob, state, context, coverage):\n",
    "        return Beam(tokens=self.tokens + [token],\n",
    "                    log_probs=self.log_probs + [log_prob],\n",
    "                    state=state,\n",
    "                    context=context,\n",
    "                    coverage=coverage)\n",
    "\n",
    "    @property\n",
    "    def latest_token(self):\n",
    "        return self.tokens[-1]\n",
    "\n",
    "    @property\n",
    "    def avg_log_prob(self):\n",
    "        return sum(self.log_probs) / len(self.tokens)\n",
    "\n",
    "\n",
    "class BeamSearch(object):\n",
    "    def __init__(self, model_file_path):\n",
    "        \n",
    "        model_name = os.path.basename(model_file_path)\n",
    "        self._test_dir = os.path.join(log_root, 'decode_%s' % (model_name))\n",
    "        self._rouge_ref_dir = os.path.join(self._test_dir, 'rouge_ref')\n",
    "        self._rouge_dec_dir = os.path.join(self._test_dir, 'rouge_dec')\n",
    "        for p in [self._test_dir, self._rouge_ref_dir, self._rouge_dec_dir]:\n",
    "            if not os.path.exists(p):\n",
    "                os.mkdir(p)\n",
    "\n",
    "        self.vocab = Vocab(vocab_path, vocab_size)\n",
    "        self.batcher = Batcher(decode_data_path, self.vocab, mode='decode',\n",
    "                               batch_size=beam_size, single_pass=True)\n",
    "        time.sleep(15)\n",
    "\n",
    "        self.model = Model(model_file_path, is_eval=True)\n",
    "\n",
    "    def sort_beams(self, beams):\n",
    "        return sorted(beams, key=lambda h: h.avg_log_prob, reverse=True)\n",
    "    \n",
    "\n",
    "    def beam_search(self, batch):\n",
    "        # single example repeated across the batch\n",
    "        enc_batch, enc_padding_mask, enc_lens, enc_batch_extend_vocab, extra_zeros, c_t, coverage = \\\n",
    "            get_input_from_batch(batch)\n",
    "\n",
    "        enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_lens)\n",
    "        s_t = self.model.reduce_state(enc_h)\n",
    "\n",
    "        dec_h, dec_c = s_t     # b x hidden_dim\n",
    "        dec_h = dec_h.squeeze()\n",
    "        dec_c = dec_c.squeeze()\n",
    "\n",
    "        # decoder batch preparation, it has beam_size example initially everything is repeated\n",
    "        beams = [Beam(tokens=[self.vocab.word2id(BOS_TOKEN)],\n",
    "                      log_probs=[0.0],\n",
    "                      state=(dec_h[0], dec_c[0]),\n",
    "                      context=c_t[0],\n",
    "                      coverage=(coverage[0] if is_coverage else None))\n",
    "                 for _ in range(beam_size)]\n",
    "\n",
    "        steps = 0\n",
    "        results = []\n",
    "        while steps < max_dec_steps and len(results) < beam_size:\n",
    "            latest_tokens = [h.latest_token for h in beams]\n",
    "            latest_tokens = [t if t < self.vocab.size() else self.vocab.word2id(UNK_TOKEN) \\\n",
    "                             for t in latest_tokens]\n",
    "            y_t = torch.LongTensor(latest_tokens)\n",
    "            y_t = y_t.to(device)\n",
    "            all_state_h = [h.state[0] for h in beams]\n",
    "            all_state_c = [h.state[1] for h in beams]\n",
    "            all_context = [h.context  for h in beams]\n",
    "\n",
    "            s_t = (torch.stack(all_state_h, 0).unsqueeze(0), torch.stack(all_state_c, 0).unsqueeze(0))\n",
    "            c_t = torch.stack(all_context, 0)\n",
    "\n",
    "            coverage_t = None\n",
    "            if is_coverage:\n",
    "                all_coverage = [h.coverage for h in beams]\n",
    "                coverage_t = torch.stack(all_coverage, 0)\n",
    "\n",
    "            final_dist, s_t, c_t, attn_dist, p_gen, coverage_t = self.model.decoder(y_t, s_t,\n",
    "                                                                                    enc_out, enc_fea,\n",
    "                                                                                    enc_padding_mask, c_t,\n",
    "                                                                                    extra_zeros, enc_batch_extend_vocab,\n",
    "                                                                                    coverage_t, steps)\n",
    "            log_probs = torch.log(final_dist)\n",
    "            topk_log_probs, topk_ids = torch.topk(log_probs, beam_size * 2)\n",
    "\n",
    "            dec_h, dec_c = s_t\n",
    "            dec_h = dec_h.squeeze()\n",
    "            dec_c = dec_c.squeeze()\n",
    "\n",
    "            all_beams = []\n",
    "            # On the first step, we only had one original hypothesis (the initial hypothesis). On subsequent steps, all original hypotheses are distinct.\n",
    "            num_orig_beams = 1 if steps == 0 else len(beams)\n",
    "            for i in range(num_orig_beams):\n",
    "                h = beams[i]\n",
    "                state_i = (dec_h[i], dec_c[i])\n",
    "                context_i = c_t[i]\n",
    "                coverage_i = (coverage[i] if is_coverage else None)\n",
    "\n",
    "                for j in range(beam_size * 2):  # for each of the top 2*beam_size hyps:\n",
    "                    new_beam = h.extend(token=topk_ids[i, j].item(),\n",
    "                                        log_prob=topk_log_probs[i, j].item(),\n",
    "                                        state=state_i,\n",
    "                                        context=context_i,\n",
    "                                        coverage=coverage_i)\n",
    "                    all_beams.append(new_beam)\n",
    "\n",
    "            beams = []\n",
    "            for h in self.sort_beams(all_beams):\n",
    "                if h.latest_token == self.vocab.word2id(EOS_TOKEN):\n",
    "                    if steps >= min_dec_steps:\n",
    "                        results.append(h)\n",
    "                else:\n",
    "                    beams.append(h)\n",
    "                if len(beams) == beam_size or len(results) == beam_size:\n",
    "                    break\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        if len(results) == 0:\n",
    "            results = beams\n",
    "\n",
    "        beams_sorted = self.sort_beams(results)\n",
    "\n",
    "        return beams_sorted[0]\n",
    "    \n",
    "    def run(self):\n",
    "        \n",
    "        counter = 0\n",
    "        start = time.time()\n",
    "        batch = self.batcher.next_batch()\n",
    "        while batch is not None:\n",
    "            # Run beam search to get best Hypothesis\n",
    "            best_summary = self.beam_search(batch)\n",
    "\n",
    "            # Extract the output ids from the hypothesis and convert back to words\n",
    "            output_ids = [int(t) for t in best_summary.tokens[1:]]\n",
    "            decoded_words = outputids2words(output_ids, self.vocab,\n",
    "                                                    (batch.art_oovs[0] if pointer_gen else None))\n",
    "\n",
    "            # Remove the [STOP] token from decoded_words, if necessary\n",
    "            try:\n",
    "                fst_stop_idx = decoded_words.index(EOS_TOKEN)\n",
    "                decoded_words = decoded_words[:fst_stop_idx]\n",
    "            except ValueError:\n",
    "                decoded_words = decoded_words\n",
    "\n",
    "            original_abstract_sents = batch.original_abstracts_sents[0]\n",
    "\n",
    "            write_for_rouge(original_abstract_sents, decoded_words, counter,\n",
    "                            self._rouge_ref_dir, self._rouge_dec_dir)\n",
    "            counter += 1\n",
    "            if counter % 1000 == 0:\n",
    "                print('%d example in %d sec' % (counter, time.time() - start))\n",
    "                start = time.time()\n",
    "\n",
    "            batch = self.batcher.next_batch()\n",
    "\n",
    "        print(\"Decoder has finished reading dataset for single_pass.\")\n",
    "        print(\"Now starting ROUGE eval...\")\n",
    "        results_dict = rouge_eval(self._rouge_ref_dir, self._rouge_dec_dir)\n",
    "        rouge_log(results_dict, self._test_dir)\n",
    "\n",
    "### utility functions\n",
    "\n",
    "def rouge_eval(ref_dir, dec_dir):\n",
    "    r = pyrouge.Rouge155()\n",
    "    r.model_filename_pattern = '#ID#_reference.txt'\n",
    "    r.system_filename_pattern = '(\\d+)_decoded.txt'\n",
    "    r.model_dir = ref_dir\n",
    "    r.system_dir = dec_dir\n",
    "    logging.getLogger('global').setLevel(logging.WARNING)  # silence pyrouge logging\n",
    "    rouge_results = r.convert_and_evaluate()\n",
    "    return r.output_to_dict(rouge_results)\n",
    "\n",
    "\n",
    "def rouge_log(results_dict, dir_to_write):\n",
    "    log_str = \"\"\n",
    "    for x in [\"1\", \"2\", \"l\"]:\n",
    "        log_str += \"\\nROUGE-%s:\\n\" % x\n",
    "        for y in [\"f_score\", \"recall\", \"precision\"]:\n",
    "            key = \"rouge_%s_%s\" % (x, y)\n",
    "            key_cb = key + \"_cb\"\n",
    "            key_ce = key + \"_ce\"\n",
    "            val = results_dict[key]\n",
    "            val_cb = results_dict[key_cb]\n",
    "            val_ce = results_dict[key_ce]\n",
    "            log_str += \"%s: %.4f with confidence interval (%.4f, %.4f)\\n\" % (key, val, val_cb, val_ce)\n",
    "    print(log_str)\n",
    "    results_file = os.path.join(dir_to_write, \"ROUGE_results.txt\")\n",
    "    print(\"Writing final ROUGE results to %s...\" % (results_file))\n",
    "    with open(results_file, \"w\") as f:\n",
    "        f.write(log_str)\n",
    "\n",
    "def write_for_rouge(reference_sents, decoded_words, ex_index,\n",
    "                    _rouge_ref_dir, _rouge_dec_dir):\n",
    "    decoded_sents = []\n",
    "    while len(decoded_words) > 0:\n",
    "        try:\n",
    "            fst_period_idx = decoded_words.index(\".\")\n",
    "        except ValueError:\n",
    "            fst_period_idx = len(decoded_words)\n",
    "        sent = decoded_words[:fst_period_idx + 1]\n",
    "        decoded_words = decoded_words[fst_period_idx + 1:]\n",
    "        decoded_sents.append(' '.join(sent))\n",
    "\n",
    "    # pyrouge calls a perl script that puts the data into HTML files.\n",
    "    # Therefore we need to make our output HTML safe.\n",
    "    decoded_sents   = [make_html_safe(w) for w in decoded_sents]\n",
    "    reference_sents = [make_html_safe(w) for w in reference_sents]\n",
    "\n",
    "    ref_file = os.path.join(_rouge_ref_dir, \"%06d_reference.txt\" % ex_index)\n",
    "    decoded_file = os.path.join(_rouge_dec_dir, \"%06d_decoded.txt\" % ex_index)\n",
    "\n",
    "    with open(ref_file, \"w\") as f:\n",
    "        for idx, sent in enumerate(reference_sents):\n",
    "            f.write(sent) if idx == len(reference_sents) - 1 else f.write(sent + \"\\n\")\n",
    "    with open(decoded_file, \"w\") as f:\n",
    "        for idx, sent in enumerate(decoded_sents):\n",
    "            f.write(sent) if idx == len(decoded_sents) - 1 else f.write(sent + \"\\n\")\n",
    "\n",
    "    # print(\"Wrote example %i to file\" % ex_index)\n",
    "    \n",
    "    def make_html_safe(s):\n",
    "        s.replace(\"<\", \"&lt;\")\n",
    "        s.replace(\">\", \"&gt;\")\n",
    "        return s\n",
    "    \n",
    "    def outputids2words(id_list, vocab, article_oovs):\n",
    "        words = []\n",
    "        for i in id_list:\n",
    "            try:\n",
    "                w = vocab.id2word(i)  # might be [UNK]\n",
    "            except ValueError as e:  # w is OOV\n",
    "                assert article_oovs is not None, \\\n",
    "                    \"Error: models produced a word ID that isn't in the vocabulary. This should not happen in baseline (no pointer-generator) mode\"\n",
    "                article_oov_idx = i - vocab.size()\n",
    "                try:\n",
    "                    w = article_oovs[article_oov_idx]\n",
    "                except ValueError as e:  # i doesn't correspond to an article oov\n",
    "                    raise ValueError(\n",
    "                        'Error: models produced word ID %i which corresponds to article OOV %i but this example only has %i article OOVs' % (\n",
    "                            i, article_oov_idx, len(article_oovs)))\n",
    "            words.append(w)\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_processor = BeamSearch(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_processor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
